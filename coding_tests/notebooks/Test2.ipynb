{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Initializing the spark context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 new artifact(s)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130 new artifacts in macro\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130 new artifacts in runtime\n",
      "130 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.spark\" %% \"spark-core\" % \"1.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkContext instance is made transient to prevent attempting to serialize it.\n",
    "\n",
    "I encountered some serialization issues when using closures referencing results from previous notebook cells.\n",
    "It seems all cell results are somehow considered as fields of the same object and Spark was trying to serialize it all (including my SparkContext).\n",
    "\n",
    "I inspired myself from: https://gist.github.com/alexarchambault/efa9e184c53fb2af6229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/05/17 23:24:23 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/05/17 23:24:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/17 23:24:23 INFO SecurityManager: Changing view acls to: mtrampont\n",
      "16/05/17 23:24:23 INFO SecurityManager: Changing modify acls to: mtrampont\n",
      "16/05/17 23:24:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mtrampont); users with modify permissions: Set(mtrampont)\n",
      "16/05/17 23:24:24 INFO Utils: Successfully started service 'sparkDriver' on port 55757.\n",
      "16/05/17 23:24:24 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/05/17 23:24:24 INFO Remoting: Starting remoting\n",
      "16/05/17 23:24:24 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.56.1:55770]\n",
      "16/05/17 23:24:24 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 55770.\n",
      "16/05/17 23:24:24 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/05/17 23:24:24 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/05/17 23:24:24 INFO DiskBlockManager: Created local directory at C:\\Users\\mtrampont\\AppData\\Local\\Temp\\blockmgr-255ce0aa-94a4-49a2-90d4-f83be9c7824b\n",
      "16/05/17 23:24:24 INFO MemoryStore: MemoryStore started with capacity 2.4 GB\n",
      "16/05/17 23:24:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/05/17 23:24:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/05/17 23:24:25 INFO SparkUI: Started SparkUI at http://192.168.56.1:4040\n",
      "16/05/17 23:24:25 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/05/17 23:24:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55807.\n",
      "16/05/17 23:24:25 INFO NettyBlockTransferService: Server created on 55807\n",
      "16/05/17 23:24:25 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/05/17 23:24:25 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55807 with 2.4 GB RAM, BlockManagerId(driver, localhost, 55807)\n",
      "16/05/17 23:24:25 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@3b1fe641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "@transient val sc = {\n",
    "  val conf = new SparkConf()\n",
    "      .setAppName(\"Test1\")\n",
    "      .setMaster(\"local[*]\")\n",
    "  SparkContext.getOrCreate(conf)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the csv file\n",
    "\n",
    "### Into an RDD\n",
    "Here is we parse the file using only spark to create an RDD with tuple containing only the fields we want (namely the airport code and the number of passengers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcsvPath\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"d:/workspace/TID_coding_tests/coding_tests/src/main/resources/bookings.csv\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val csvPath = \"d:/workspace/TID_coding_tests/coding_tests/src/main/resources/bookings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the header line and the columns indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[36mheaderLine\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      \"\u001b[0m\n",
       "\u001b[36mcolNames\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"off_port\"\u001b[0m -> \u001b[32m21\u001b[0m,\n",
       "  \u001b[32m\"duration\"\u001b[0m -> \u001b[32m7\u001b[0m,\n",
       "  \u001b[32m\"brd_ctry\"\u001b[0m -> \u001b[32m20\u001b[0m,\n",
       "  \u001b[32m\"lst_city\"\u001b[0m -> \u001b[32m16\u001b[0m,\n",
       "  \u001b[32m\"pos_ctry\"\u001b[0m -> \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m\"source\"\u001b[0m -> \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m\"mkt_port\"\u001b[0m -> \u001b[32m24\u001b[0m,\n",
       "  \u001b[32m\"carrier\"\u001b[0m -> \u001b[32m29\u001b[0m,\n",
       "  \u001b[32m\"arr_ctry\"\u001b[0m -> \u001b[32m14\u001b[0m,\n",
       "  \u001b[32m\"pos_oid\"\u001b[0m -> \u001b[32m4\u001b[0m,\n",
       "  \u001b[32m\"lst_ctry\"\u001b[0m -> \u001b[32m17\u001b[0m,\n",
       "  \u001b[32m\"oid\"\u001b[0m -> \u001b[32m37\u001b[0m,\n",
       "  \u001b[32m\"arr_city\"\u001b[0m -> \u001b[32m13\u001b[0m,\n",
       "  \u001b[32m\"cre_date\"\u001b[0m -> \u001b[32m6\u001b[0m,\n",
       "  \u001b[32m\"year\"\u001b[0m -> \u001b[32m35\u001b[0m,\n",
       "  \u001b[32m\"mkt_ctry\"\u001b[0m -> \u001b[32m26\u001b[0m,\n",
       "  \u001b[32m\"cab_class\"\u001b[0m -> \u001b[32m31\u001b[0m,\n",
       "  \u001b[32m\"rloc\"\u001b[0m -> \u001b[32m5\u001b[0m,\n",
       "  \u001b[32m\"dep_city\"\u001b[0m -> \u001b[32m10\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val headerLine =\n",
    "  Source.fromFile(csvPath)\n",
    "        .getLines()\n",
    "        .take(1)\n",
    "        .toList\n",
    "        .head\n",
    "\n",
    "val colNames =\n",
    "  headerLine.split(\"\"\"\\^\"\"\", -1)\n",
    "      .map(_.trim)\n",
    "      .zipWithIndex\n",
    "      .toMap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can parse the file and create a RDD[(String, Int)] with the arrival airport and the number of passengers per booking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbookings\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = MapPartitionsRDD[9] at map at Main.scala:34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val bookings =\n",
    "      sc.textFile(csvPath)\n",
    "        .filter(_ != headerLine) // The first partition will get the header line. We need to discard it\n",
    "        .map{ record =>\n",
    "          val fields = record.split(\"\"\"\\^\"\"\", -1).map(_.trim)\n",
    "          (fields(colNames(\"arr_port\")), fields(colNames(\"pax\")).toInt)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we only defined transformations. Let's try an action to test the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 63 in stage 2.0 failed 1 times, most recent failure: Lost task 63.0 in stage 2.0 (TID 67, localhost): java.lang.ArrayIndexOutOfBoundsException: 34\r",
      "\tat cmd9$$user$$anonfun$1$$anonfun$apply$2.apply(Main.scala:36)\r",
      "\tat cmd9$$user$$anonfun$1$$anonfun$apply$2.apply(Main.scala:34)\r",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1597)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1143)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1143)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r",
      "\tat java.lang.Thread.run(Unknown Source)\r",
      "",
      "Driver stacktrace: (Job aborted due to stage failure: Task 63 in stage 2.0 failed 1 times, most recent failure: Lost task 63.0 in stage 2.0 (TID 67, localhost): java.lang.ArrayIndexOutOfBoundsException: 34\r",
      "\tat cmd9$$user$$anonfun$1$$anonfun$apply$2.apply(Main.scala:36)\r",
      "\tat cmd9$$user$$anonfun$1$$anonfun$apply$2.apply(Main.scala:34)\r",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1597)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1143)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1143)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r",
      "\tat java.lang.Thread.run(Unknown Source)\r",
      "",
      "Driver stacktrace:)",
      "  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)",
      "  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)",
      "  scala.Option.foreach(Option.scala:257)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)",
      "  org.apache.spark.rdd.RDD.count(RDD.scala:1143)",
      "  cmd10$$user$$anonfun$1.apply$mcJ$sp(Main.scala:25)",
      "java.lang.ArrayIndexOutOfBoundsException: 34 (34)",
      "  cmd9$$user$$anonfun$1$$anonfun$apply$2.apply(Main.scala:36)",
      "  cmd9$$user$$anonfun$1$$anonfun$apply$2.apply(Main.scala:34)",
      "  scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "  org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1597)",
      "  org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1143)",
      "  org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1143)",
      "  org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)",
      "  org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)",
      "  org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)",
      "  org.apache.spark.scheduler.Task.run(Task.scala:89)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)",
      "  java.lang.Thread.run(Unknown Source)"
     ]
    }
   ],
   "source": [
    "bookings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like parsing of some records failed.\n",
    "Let's try a version with more errors handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalArgumentException: Parsing failed with line: 2013-03-25 00:00:00^1V    JP      ^^a37584d1485cb35991e4ff1a2ba92262^2013-03-25 00:00:00^8371^60^NRT     ^TYO     ^JP      ^SIN     ^SIN     ^SG      ^HND     TYO     ^JP      ^NRT     ^TYO     ^JP      ^SIN     ^SIN     ^SG      ^NRTSIN  ^SINTYO  ^JPSG    ^1^NRTSIN         ^XR,Q        ^Y        ^2013-04-14 11:05:00^2013-04-14 17:10:56^2^2013^3^NULL     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.util.{Try, Failure, Success}\u001b[0m\n",
       "\u001b[36mbookings\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mTry\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)]] = MapPartitionsRDD[17] at map at Main.scala:40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.util.{Try, Failure, Success}\n",
    "\n",
    "val bookings =\n",
    "  sc.textFile(csvPath)\n",
    "    .filter(_ != headerLine)\n",
    "    .map{ record =>\n",
    "      Try{\n",
    "        val fields = record.split(\"\"\"\\^\"\"\", -1).map(_.trim)\n",
    "        (fields(colNames(\"arr_port\")), fields(colNames(\"pax\")).toInt)\n",
    "      }.recoverWith{ case cause =>\n",
    "        Failure(new IllegalArgumentException(s\"Parsing failed with line: $record\", cause))\n",
    "      }\n",
    "    }\n",
    "\n",
    "bookings.collect{case Failure(cause) => cause}\n",
    "    .collect()\n",
    "    .foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems only one record has some issues. Some fields and separators are missing, and there is even a \",\" that I guess should be a separator.\n",
    "Let's discard it and continue with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36marrivalAirports\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = ShuffledRDD[22] at aggregateByKey at Main.scala:27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val arrivalAirports =\n",
    "  bookings.collect{case Success(bkg) => bkg}\n",
    "      .aggregateByKey(0)(_+_, _+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sums the passengers by airport.\n",
    "\n",
    "We can then sort the resulting RDD and take the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"LHR\"\u001b[0m, \u001b[32m88809\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"MCO\"\u001b[0m, \u001b[32m70930\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"LAX\"\u001b[0m, \u001b[32m70530\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"LAS\"\u001b[0m, \u001b[32m69630\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"JFK\"\u001b[0m, \u001b[32m66270\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CDG\"\u001b[0m, \u001b[32m64490\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"BKK\"\u001b[0m, \u001b[32m59460\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"MIA\"\u001b[0m, \u001b[32m58150\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"SFO\"\u001b[0m, \u001b[32m58000\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"DXB\"\u001b[0m, \u001b[32m55590\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrivalAirports.sortBy(_._2, false).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we need only the top 10, we can use `top`, which will save us from sorting the whole RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres16\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"LHR\"\u001b[0m, \u001b[32m88809\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"MCO\"\u001b[0m, \u001b[32m70930\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"LAX\"\u001b[0m, \u001b[32m70530\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"LAS\"\u001b[0m, \u001b[32m69630\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"JFK\"\u001b[0m, \u001b[32m66270\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CDG\"\u001b[0m, \u001b[32m64490\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"BKK\"\u001b[0m, \u001b[32m59460\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"MIA\"\u001b[0m, \u001b[32m58150\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"SFO\"\u001b[0m, \u001b[32m58000\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"DXB\"\u001b[0m, \u001b[32m55590\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrivalAirports.top(10)(Ordering.by[(String, Int),Int](_._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

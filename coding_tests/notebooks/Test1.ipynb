{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 new artifact(s)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130 new artifacts in macro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "130 new artifacts in runtime\n",
      "130 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.spark\" %% \"spark-core\" % \"1.6.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/05/17 11:45:34 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/05/17 11:45:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/17 11:45:34 INFO SecurityManager: Changing view acls to: mtrampont\n",
      "16/05/17 11:45:34 INFO SecurityManager: Changing modify acls to: mtrampont\n",
      "16/05/17 11:45:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mtrampont); users with modify permissions: Set(mtrampont)\n",
      "16/05/17 11:45:35 INFO Utils: Successfully started service 'sparkDriver' on port 57685.\n",
      "16/05/17 11:45:35 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/05/17 11:45:35 INFO Remoting: Starting remoting\n",
      "16/05/17 11:45:35 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.56.1:57700]\n",
      "16/05/17 11:45:35 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 57700.\n",
      "16/05/17 11:45:35 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/05/17 11:45:35 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/05/17 11:45:35 INFO DiskBlockManager: Created local directory at C:\\Users\\mtrampont\\AppData\\Local\\Temp\\blockmgr-3c0134b0-1a3b-4ca5-85ba-b857151ce65d\n",
      "16/05/17 11:45:35 INFO MemoryStore: MemoryStore started with capacity 2.4 GB\n",
      "16/05/17 11:45:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/05/17 11:45:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/05/17 11:45:35 INFO SparkUI: Started SparkUI at http://192.168.56.1:4040\n",
      "16/05/17 11:45:35 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/05/17 11:45:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57737.\n",
      "16/05/17 11:45:35 INFO NettyBlockTransferService: Server created on 57737\n",
      "16/05/17 11:45:35 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/05/17 11:45:35 INFO BlockManagerMasterEndpoint: Registering block manager localhost:57737 with 2.4 GB RAM, BlockManagerId(driver, localhost, 57737)\n",
      "16/05/17 11:45:35 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@5698e9c"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "val sc = {\n",
    "  val conf = new SparkConf()\n",
    "      .setAppName(\"Test1\")\n",
    "      .setMaster(\"local[*]\")\n",
    "  SparkContext.getOrCreate(conf)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbookingsRecords\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = MapPartitionsRDD[3] at textFile at Main.scala:25\n",
       "\u001b[36msearchesRecords\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = MapPartitionsRDD[5] at textFile at Main.scala:28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val bookingsRecords = sc.textFile(\"d:/workspace/TID_coding_tests/coding_tests/src/main/resources/bookings.csv\")\n",
    "val searchesRecords = sc.textFile(\"d:/workspace/TID_coding_tests/coding_tests/src/main/resources/searches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NbBookings: 10000011\n",
      "NbSearches: 20390199\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(s\"NbBookings: ${bookingsRecords.count()}\")\n",
    "println(s\"NbSearches: ${searchesRecords.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
